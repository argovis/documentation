.. _new_data:

Serving a new dataset
=====================

Argovis developed and implements semi-standardized data schema and API routes for *point-like* Earth system data. Any data that can be expressed as JSON documents each associated with a specific latitude, longitude and timestamp can be represented in this schema, and a search and filtering API created to parse it. In this document, we'll tour the standard procedure for providing Argovis-standard API endpoints across a new dataset.

1. Define the data and metadata schema
--------------------------------------

All standard data in Argovis is described by schemas compliant with :ref:`schema`, which describes Earth system data as pairs of data and metadata documents. Your first job with a new dataset is creating a compliant schema appropriate for your data.

1. Review :ref:`schema` to understand the schema requirements, and see examples of customizations for different datasets.

2. Identify an organizing principle for your metadata documents. As discussed in the above doc, data documents should capture information that changes with each temporospatial point, while metadata documents should capture information that is common to many such points. A common organizing principle is physical platform, as seen in Argo floats and Global Drifter Program drifters; these devices have settings and parameters that are common for hundereds or thousands of measurements. CCHDO, on the other hand, uses cruises as its metadata organizing principle - organized campaigns in time that generated a series of measurements with common metadata. Your dataset will be different; try and choose something that is both logically meaningful to subject matter experts in that dataset like these examples, and that generates a many-to-one relationship between data and metadata documents.

3. Once you've decided on your data schema, implement it in `https://github.com/argovis/db-schema <https://github.com/argovis/db-schema>`_, following one of the existing examples. 

4. In the above script, be sure to also codify the database indexes you want for both data and metadata collections. Metadata collections can be indexed in any way you intend to search them; data collections must at a minimum be indexed by ``metadata``, ``timestamp``, ``timestamp`` x ``geolocation``, and ``geolocation``; see the ``tc`` schema for a simple example.

5. Build and run your containerized script on the cluster; if all's well, there should be a data and a metadata collection defined in mongodb, with indexes for your dataset.

2. Populate your collections
----------------------------

Once collections are defined, they need to be populated from their upstream raw data. A relatively straightforawrd example of this can be seen for the tropical cyclone data in `https://github.com/argovis/tc-sync <https://github.com/argovis/tc-sync>`_. Beyond basic collection population, make sure there's enough documentation to rebuild the dataset from scratch, and for complex datasets, consider an intrgrity check script that can run in the background after the collections are built, and doublecheck that what's in the database matches your upstream data origin; see `the Argo roundtrip watchdog <https://github.com/argovis/ifremer-sync/blob/main/roundtrip.yaml>`_ and downstream scripts for an example of this.

3. Establish standard API routes
--------------------------------

All standard collections in our database offer the following routes:

 - ``/<dataset>``: search and filter for the data documents you defined above
 - ``/<dataset>/meta``: search and filter for the metadata documents you defined above
 - ``/<dataset>/vocabulary`` (optional): vocabulary route to enumerate possible values of search terms of the above two routes, such as names of tropical cyclones for the ``tc`` routes, or WOCE lines for the ``cchdo`` routes. 

The steps in this section enumerate how to set these up.

API templates
+++++++++++++

On the ``templates`` branch of `https://github.com/argovis/argovis_api <https://github.com/argovis/argovis_api>`_ in ``spec.yaml``, the OpenAPI specifications of your API endpoints are defined. For all of the following, see the tropical cyclone (``/tc``) implementation in the existing code for a simple example.

 - ``/<dataset>``: mandatory query string parameters: genericID, startDate, endDate, polygon, multipolygon, center, radius, mostrecent and compression. Add extra search parameters from the data or metadata collections as desired, but note that each custom query filter *must* have its corresponding variable individually indexed in mongo.
 - ``/<dataset>/meta``: the only mandatory query string parameter is genericID, and as above, add any others you like, but ensure they are indexed.
 - ``/<dataset>/vocab`` (optional): has exactly one query string parameter called ``parameter``, with allowed values equal to ``data_keys`` and the query string parameters from the other two canonical routes which you'd like to offer enumerations of allowed values for. Again, see the tc example, where ``parameter=name`` is used to list all the tropical cyclone names the tc data and metadata can be searched by.
 
In addition to the route definitions, the 200 responses of ``/<dataset>`` and ``/<dataset>/meta`` need valid response schemas defined. These are almost identical to the mongo schema you defined above, with the following exceptions:

 - ``data`` should be optional in the response schema for ``/<dataset>``, while mandatory in the mongo data collection documents. In mongo, data collection documents clearly need the data they represent; but the ``/<dataset>`` API routes may return a response absent this key, for coordinate-only searches.
 - If the mongo schema define ``data_keys`` and ``units`` to be on the metadata documents, they should be allowed, but not required, in the ``/<dataset>`` responses. This is because requests to ``</dataset>`` that subset on data variables must carry a custom value of ``data_keys`` and ``units`` that supercede the metadata document values to reflect user choises. 

API build
+++++++++

Once you've created the templates for your new API endpoints as per the previous section, follow the instructions in `https://github.com/argovis/argovis_api#development-cycle <https://github.com/argovis/argovis_api#development-cycle>`_ to merge those templates into the ``server`` branch and begin implementing the logic to handle them. When following the step in those instructions that says 'Implement custom logic', consider the following:

 - First, update the controller for your new routes. The patterns for all of these are identical; as usual, see the tropical cyclone implementation for a simple example.
 
 - Add models for your new data and metadata documents, and point to their collections.

 - The most sophisticated step is to implement the service for your new endpoints, which captures the bulk of the logic on how these requests are handled. Again, start with the tropical cyclone example as a template, and be sure to insert the correct references to your data and metadata models. 

 - If you want some custom logic on your new routes beyond the simple TC patterns, have a look at the Argo service example. In this example, Argo supports a ``source`` parameter, and Argo's data service implements some extra custom input validation and filtering to handle this.

 - Finally, now is the time to add some tests for your API logic, at a minimum checking that basic requests and filters return what's expected. To do so, see the test database in `https://github.com/argovis/testdb <https://github.com/argovis/testdb>`_; add some data and corresponding metadata documents to ``seed-data.js`` there, build and push a new ``argovis/testdb:0.xx`` container image, and update ``.travis.yml`` in the API to use this new test database version.

Release
+++++++

Once API logic is implemented and tests are passing on the ``server`` branch of the API repo, it's time to release:

 - Follow the release process for the API, documented here: `https://github.com/argovis/argovis_api#build--release-cycle <https://github.com/argovis/argovis_api#build--release-cycle>`_.

 - Follow the release process for the deployment manager, documented here: `https://github.com/argovis/argovis_deployment#release-process <https://github.com/argovis/argovis_deployment#release-process>`_.

At this point, your new data should be reachable from your new API endpoints.

4. Documentation
----------------

The final step for your new dataset is to provide some documentation for it. Swagger will have automatically generated and posted the basic API documentation for your new endpoints, but there are a couple other places that should be highlighted:

 - Describe the schema decisions for your dataset in `https://github.com/argovis/documentation/blob/main/data_management/schema.rst <https://github.com/argovis/documentation/blob/main/data_management/schema.rst>`_; be sure to also link out to any data parsing and upload scripts you developed as well.

 - (Optional): Consider illustrating any novel usage or applications of your new dataset in a demo notebook at `https://github.com/argovis/demo_notebooks <https://github.com/argovis/demo_notebooks>`_.

 *Last reviewed 2022-09-14*