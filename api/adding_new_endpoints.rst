.. _adding_new_routes:

Adding new API routes
=====================

Due to the heavy standardization of Argovis' schema and API endpoints, creating the endpoints for a new dataset is a mostly wrote procedure. This document enumerates the steps to add the standard routes in front of the new data and metadata collections you created by following the procedures outlined in :ref:`adding_new_data`.

Updating the OpenAPI spec
-------------------------

Argovis API code is maintained in `https://github.com/argovis/argovis_api <https://github.com/argovis/argovis_api>`_ on two branches:

 - ``templates``, which contains the OpenAPI specifications in ``*-spec.json``, and all the auto-generated boilerplate code for the NodeJS implementation of each route
 - ``server``, which modifies the autogenerated code with bespoke business logic for Argovis.

Our first step is to update the appropriate ``*-spec.json`` on the ``templates`` branch. **Specification JSONs should only be modified on the templates branch, and only specification JSONs should be modified by hand on the templates branch**. All other files on this branch are auto-generated.

Look in ``core-spec.json`` for the three routes ``/tc``, ``/tc/meta`` and ``/tc/vocabulary``; these provide good first examples of minimal mandatory routes. The following parameters are mandatory for all data routes: ``genericID``, ``startDate``, ``endDate``, ``polygon``, ``center``, ``radius``, ``metadata``, ``mostrecent`` and ``compression``; these provide generic temporospatial, metadata and representation search and transform options that will be provided by generic helper functions when we get to the business logic in a later step; feel free to add any other query string parameters you want to let users seatch by, that correspond to things you indexed when you defined your collection schema. Similarly, ``meta`` routes must at a minimum have the ``genericID`` query string parameter, and ``vocabulary`` routes must have the ``parameter`` query string parameter.

Also see examples of custom routes like ``/argo/bgc``, which may optionally be defined in addition to the conventional routes. These are more freeform and case-by-case routes, often used to return documents from the ``summaries`` collection.

Note that at the time of writing, highly regularized collections like timeseries and extended objects all have exactly one metadata document per collection, and that metadata document has ``_id`` property identical to the route name; you'll need to consider this when specifying your routes.

Note that near the top of this specification, there is a ``tags`` section. Add a tag here that labels your new dataset, if necessary, and use it to group your API endpoints together; this will affect how they are listed in the auto-generated Swagger documentation. Those tags are where to add the ``(experimental)`` note if you're writing API endpoints outside the usual structure of standard Argovis routes.

Once you've written your JSON description of your new routes and return schemas, look in the ``README.md`` file in the API repo for build instructions for the ``templates`` branch. Build the boilerplate with the conventions you find there, make a new templates development branch (perhaps ``templates-dev``, or something more specific), and add and commit your new files to this branch.

Updating server logic
---------------------

At this point, you've updated the specification for your API manually and boilerplate automatically; now it's time to write the Argovis-specific logic. This logic lives on the ``server`` branch. Check this branch out, make a new dev branch (perhaps ``server-dev``, or something more specific), and merge in your ``templates-dev`` to get started.

Controller logic
++++++++++++++++

Have a look in ``nodejs-server/controllers``. The build process from the templates creates a controller for each tag listed above. Again, look at the tropical cyclone example ``Tc.js`` for a simple example. The controller logic for data, metadata and vocab routes is very generic and handled by the helper functions you see called there; include the helper script in your new controller and update the boilerplate for these three route controllers in close analogy to the tropical cyclone example.

Mongoose model
++++++++++++++

Next, we need to define a model for our data that mongoose, our nodeJS MongoDB interface, can parse, for both data and metadata objects, in ``nodejs-server/models``. This should be similar to the schema you described when defining your collection, though note the ``data`` key on data documents is always optional at the API level; Argovis allows returning everything *but* the data key from a data document for mapping and metadata analysis purposes. Again, the tropical cyclone example provides a good simple example you can modify to meet your needs.
 
Service logic
+++++++++++++

The most complex API logic lives in the service definitions under ``nodejs-server/service``; again, there will be one boilerplate file generated per API tag, and also once again the tropical cyclone service provides a good starting point for the custom logic. Copy the guts of the ``findTC`` and ``findTCmeta`` functions into the data and metadata service functions for your new dataset, and look out for the following customizations:

 - In the 'local filter' section, you'll add keys to filter data docs other than temporospatial ones by. If you added custom query parameters to your data route that filter on data document values, this is where they get consumed and turned into a database search.
 - The 'metadata table filter' section similarly facilitates filtering down on parameters found in the metadata documents.
 - Be sure to import the appropriate mongoose model, and search the correct collection, following the cyclone example.
 - You may write a custom ``stub`` function in the callback to the database query. This should be a function that translates data and metadata document pairs into a *very* minimal stub, including geolocation, timestamp and ID, and not much else - typically just what is absolutely needed for mapping purposes.

Metadata and vocabulary service functions are much simpler; respectively, they in most cases do a simple match on an indexed metadata key, or return a single summary document prepared beforehand in the database creation step.

Token bucket logic
++++++++++++++++++

As discussed further in :ref:`api_rate_limit`, user requests to many of Argovis' API endpoints are capped by a token bucket algorithm. When implementing new routes, consider the potential for large or cumbersome requests: any collection with global coverage and / or long chronological extent, at a minimum, needs to be included in the standard rate limiting logic, which can be done by modifying the ``cost`` function in ``nodejs-server/helpers/helpers.js``. For a new dataset, at least:

 - Update the ``earliest_records`` and ``final_records`` functions with the first and last ``timestamp`` found on documents in your data collection.
 - Update ``standard_routes`` with the first path component of your new routes.

Think carefully at this step about the queries you have allowed via your indexes and query string parameters: are there any that could trigger unindexed lookups, or which could return many (> 1000) documents? You may wish to reconsider allowing these at all, but if you must allow them, at least apply a high token cost to the requests so that they can't be made very rapidly.

Special cases
+++++++++++++

There are a few special cases to tend to, especially around grid and timeseries:

 - For timeseries, don't miss indicating which collections are timeseries in the ``helpers.datatable_stream`` function, as these are handled a little differently due to their schema differences.
 - Also for timeseries, don't miss updating the collection dictionary in the vocab route logic.
 - For grids managed by the ``grids/`` routes, don't miss updating ``helpers.find_grid_collection``.

Testing
-------

Once server API logic is complete, the last code to develop is a suite of unit tests for your new dataset and endpoints. Have a look in ``.travis.yml`` for the test procedure, and under ``tests/tests`` for example test logic.

Mock database update
++++++++++++++++++++

API unit tests run against a mock database, which is a MongoDB instance with a few documents from each collection hard-coded into the database container. Have a look at `https://github.com/argovis/testdb <https://github.com/argovis/testdb>`_ for the source for the test database; add a few data documents from your new collection along with their corresponding metadata documents, and build and push a container that increments the ``argovis/testdb`` image tag.

Test cases
++++++++++

Create a new file in the appropriate sub-folder of ``tests/tests`` for your test cases, copying the tropical cyclone example as a starting place. Write tests that cover the following cases:

 - Make sure all individual query string parameters return what they're supposed to.
 - Check a few combinations of query string parameters that influence each other - for example, if you use a ``data`` filter, do the right keys appear in ``data_info``? This is especially important if any of your dataset-specific query string parameters have interactions of this nature.
 - Check that returned documents match the schema you specified in your JSON specifications.

As well, of course, as any other behavior you expect to be complex or generate edge cases. As with any software project, if bugs are identified in future, this test suite should be expanded with unit tests that demonstrate the bug and check for its regression in future.

Local testing
+++++++++++++

The test procedure described in ``travis.yml`` is simple enough that you can run it locally with Docker installed on your development laptop. Do so now, and ensure tests are passing before pushing your ``server-dev`` and ``templates-dev`` branches (or equivalent) to GitHub.

Build & release
---------------

Once you believe development is complete and tests are passing locally, it's time to set up pull requests and organize a new release of Argovis' API.

1. **Pull requests:** set up PRs from your ``templates-dev`` branch to ``templates``, and from your ``server-dev`` branch to ``server``. Merge only when travis signs off on all tests passing on the server branch.
2. **Code release:** create a new release of the API repository from the ``server`` branch. Tag it with a semantic version number, and include in the release notes a changelog listing the PRs and changes since the previous release.
3. **Container release:** build a container from the newly-released ``server`` branch code, tagged as ``argovis/api:major.minor.patch`` to match the release tag, and push to Docker Hub.
4. **Production release:** update production deployments to use the latest API image. See `https://github.com/argovis/argovis_deployment <https://github.com/argovis/argovis_deployment>`_ for Helm chart and Docker Swarm deployment manifests to update and run.
5. **Acknowledgements:** once your new dataset and API are live in production, make sure to add an acknowledgement to `https://github.com/argovis/react/blob/main/argovis/src/pages/about.jsx <https://github.com/argovis/react/blob/main/argovis/src/pages/about.jsx>`_, including DOIs and citations as appropriate.

Argovis helpers
---------------

Once API launch is complete, consider the necessity to update `https://github.com/argovis/argovis_helpers <https://github.com/argovis/argovis_helpers>`_, the pythonic helper package for Argovis. At a minimum, you'll want to tell that package to slice up temporospatially large requests for your new collection in its ``query`` function; consider possible advantages of adding or updating other helpers simultaneously. Also, consider if these new helpers or API endpoints would do well to be illustrated in a demo in Argovis' collection of `jupyter notebooks <https://github.com/argovis/demo_notebooks>`_.

*Last reviewed 24-04-11*